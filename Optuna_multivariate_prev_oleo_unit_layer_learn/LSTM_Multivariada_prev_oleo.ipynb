{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tentativa de Previsão de Oleo com base em dados Multivariados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carregando libs necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "# from keras.layers import SimpleRNN, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.layers import GRU, Dense,Dropout,LSTM\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler,LabelEncoder\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.metrics import mean_absolute_error , mean_squared_error\n",
    "import optuna\n",
    "# pip install optuna\n",
    "!pip install optuna-integration\n",
    "\n",
    "\n",
    "from time import time_ns\n",
    "import random\n",
    "import keras\n",
    "\n",
    "SEED = 10\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Função para a preparação dos dados para o modelo de previsão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function is defined with default parameters so that if you call it with just your data, it will construct a DataFrame with t-1 \n",
    "# as X and t as y\n",
    "\n",
    "def series_to_supervised(data, n_in = 1, n_out = 1, dropnan = True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    \n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "        \n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "            \n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "        \n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carregando a base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "file_name = r\"C:\\Users\\vinicius\\Documents\\Repositorios\\TimeSeriesPredictionFinalProject\\src\\data\\Base_Volve_Pocos.xlsx\" # File name\n",
    "###########################################################\n",
    "\n",
    "#MELHOR PARA TREINAR E FICAR BOM: 'NO15-9-F-1-C'>'NO15-9-F-15-D' -> Esse cara vai muito para zero\n",
    "# BOM MAS COM PERFIL SHIFITADO 'NO15-9-F-11-H'~=~'NO15-9-F-12-H'#\n",
    "sheet_name ='NO15-9-F-1-C' #'NO15-9-F-12-H'# 'NO15-9-F-14-H'#'NO15-9-F-15-D'#'NO15-9-F-11-H'#'NO15-9-F-12-H'#'NO15-9-F-11-H'# 4th sheet\n",
    "header = 0 # The header is the 2nd row\n",
    "df_full = pd.read_excel(file_name, sheet_name = sheet_name, header = header)\n",
    "df_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificação se todos os dados da base são referentes a um poço produtor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter=df_full[df_full['WELL_TYPE'] != 'WI']\n",
    "df_filter.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remoção de colunas desnecessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter=df_filter.drop(columns=['WELL_BORE_CODE','NPD_WELL_BORE_CODE', 'NPD_WELL_BORE_NAME',\n",
    "                                  'NPD_FIELD_CODE','NPD_FIELD_NAME',\n",
    "                                  'NPD_FACILITY_CODE','NPD_FACILITY_NAME',\n",
    "                                  'AVG_CHOKE_UOM','FLOW_KIND',\n",
    "                                  'WELL_TYPE','BORE_WI_VOL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot de todos os dados da base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = df_filter.values\n",
    "\n",
    "# specify columns to plot\n",
    "groups = [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "i = 1\n",
    "colors=['blue','salmon','deeppink',\"forestgreen\",'red','orange','darkviolet','lime','gold','indigo','tomato','navy']\n",
    "\n",
    "# plot each column\n",
    "plt.figure(figsize=(30,20))\n",
    "for group in groups:\n",
    "    plt.subplot(len(groups), 1, i)\n",
    "    plt.plot(values[:, group], c = colors[i])\n",
    "    plt.title(df_filter.columns[group], y=0.75, loc='right', fontsize = 18)\n",
    "    i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtro para selecionar apenas as features desejadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df_filter[['BORE_OIL_VOL','BORE_WAT_VOL','BORE_GAS_VOL','AVG_WHP_P','AVG_WHT_P']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = target.values\n",
    "\n",
    "# specify columns to plot\n",
    "groups = [0, 1, 2, 3, 4]\n",
    "i = 1\n",
    "# plot each column\n",
    "plt.figure(figsize=(20,14))\n",
    "for group in groups:\n",
    "    plt.subplot(len(groups), 1, i)\n",
    "    plt.plot(values[:, group], c = \"forestgreen\")\n",
    "    plt.title(target.columns[group], y=0.75, loc='right', fontsize = 15)\n",
    "    i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualização do perfil completo da produção de Oleo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,7))\n",
    "plt.plot(target.BORE_OIL_VOL)\n",
    "plt.title(\"Produção de óleo por dia\", fontsize = 15)\n",
    "plt.xlabel('tempo [dias]', fontsize = 15)\n",
    "plt.ylabel(r'Produção de Óleo [$m^3$/dia]', fontsize = 15)  # LaTeX formatting for the y-label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap da correlação entre os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(target.corr(),annot=True, cbar=False, cmap='Blues', fmt='.3f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparação da base de dados para criação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the entire dataset\n",
    "\n",
    "dataset = target\n",
    "values = dataset.values\n",
    "\n",
    "values = values.astype('float32')\n",
    "\n",
    "# Fazendo uma normalização nos dados para ficarem entre 0 e 1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the dataset as supervised learning\n",
    "\n",
    "reframed = series_to_supervised(scaled, 4, 1) # 7,10 ficou horrivel # Ficou melhorzinho 4>5>1>3>2\n",
    "print(reframed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(reframed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualização das novas colunas criadas no processo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reframed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Como o objetivo desse notebook é a previsão apenas de óleo, vamos remover as outras colunas no instante t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping columns we don't want to predict\n",
    "columns_to_drop = ['var2(t)', 'var3(t)', 'var4(t)', 'var5(t)']\n",
    "reframed.drop(columns_to_drop, axis=1, inplace=True)\n",
    "display(reframed.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos definir o percentual de dados que vai ser utilizada no treino e no teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "split_index = int(len(target) * train_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Separação dos dados de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = reframed.values\n",
    "\n",
    "# We train the model on the 1st 3 years and then test on the last year (for now)\n",
    "n_train_hours = split_index\n",
    "\n",
    "train = values[:split_index, :]\n",
    "test = values[split_index:, :]\n",
    "\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "\n",
    "# reshape input to be 3D :- (no.of samples, no.of timesteps, no.of features)\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definição da quantidade de neuronios que vão ser utilizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = 50 \n",
    "n_neurons\n",
    "epochs = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.integration import TFKerasPruningCallback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a callback to save information\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, trial):\n",
    "        self.trial = trial\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_model = None\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss = logs['val_loss']\n",
    "        if val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.best_model = self.model\n",
    "        self.trial.report(val_loss, step=epoch)\n",
    "        if self.trial.should_prune():\n",
    "            raise optuna.TrialPruned()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define the Optuna search space for hyperparameters\n",
    "    n_lstm_layers = trial.suggest_int('n_lstm_layers', 1, 10)\n",
    "    lstm_units = trial.suggest_int('lstm_units', 32, 256)\n",
    "    learning_rate = 1e-4#trial.suggest_float('learning_rate', 1e-4, 1e-2)\n",
    "    \n",
    "    # Create the model with the suggested hyperparameters\n",
    "    model_LSTM = Sequential()\n",
    "    model_LSTM.reset_states()\n",
    "    \n",
    "    for _ in range(n_lstm_layers):\n",
    "        model_LSTM.add(LSTM(lstm_units, input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=True))\n",
    "    \n",
    "    model_LSTM.add(LSTM(lstm_units, return_sequences=False))\n",
    "    model_LSTM.add(Dense(1))\n",
    "    \n",
    "    # Compile the model with the suggested learning rate\n",
    "    model_LSTM.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=learning_rate), metrics=['mean_absolute_error'])\n",
    "\n",
    "    # Create a custom callback for saving information\n",
    "    custom_callback = CustomCallback(trial)\n",
    "    \n",
    "    # Train the model with your data and use the custom callback\n",
    "    history = model_LSTM.fit(train_X, train_y, epochs=epochs, batch_size=1, validation_data=(test_X, test_y), callbacks=[custom_callback], verbose=0)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    \n",
    "    # Save the model as a pkl file\n",
    "    model_filename = f\"model_multivariable_layer_unit{trial.number}.pkl\"\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(model_LSTM, model_file)\n",
    "    \n",
    "    return val_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction='minimize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the model using Optuna\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "best_loss = study.best_value\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Validation Loss:\", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_units = study.best_params['lstm_units']#trial.suggest_int('lstm_units', 32, 256)\n",
    "learning_rate = 1e-4#trial.suggest_float('learning_rate', 1e-4, 1e-2)\n",
    "best_model_LSTM = Sequential()\n",
    "best_model_LSTM.reset_states()\n",
    "    \n",
    "for _ in range(study.best_params['n_lstm_layers']):\n",
    "    best_model_LSTM.add(LSTM(lstm_units, input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=True))\n",
    "    \n",
    "best_model_LSTM.add(LSTM(lstm_units, return_sequences=False))\n",
    "best_model_LSTM.add(Dense(1))\n",
    "\n",
    "# Compile the model with the suggested learning rate\n",
    "best_model_LSTM.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=learning_rate), metrics=['mean_absolute_error'])\n",
    "\n",
    "    \n",
    "# Train the model with your data and use the custom callback\n",
    "history = best_model_LSTM.fit(train_X, train_y, epochs=epochs, batch_size=1, validation_data=(test_X, test_y), verbose=0)\n",
    "    \n",
    "# Evaluate the model\n",
    "train_loss_history_LSTM = history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_LSTM = Sequential()\n",
    "# model_LSTM.reset_states()\n",
    "# model_LSTM.add(LSTM(n_neurons, input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=True))\n",
    "# model_LSTM.add(LSTM(n_neurons, return_sequences=True))\n",
    "# model_LSTM.add(LSTM(n_neurons, return_sequences=False))\n",
    "# model_LSTM.add(Dense(1))\n",
    "# model_LSTM.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "# model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definindo a quantiade de epocas e informando que todos os dados serão utilizados no processo de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 300 # Choose the number of training epochs\n",
    "batch_size = 1#5#10 # Choose the batch size for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model\n",
    "# history_LSTM = model_LSTM.fit(train_X, train_y, batch_size=batch_size, epochs=epochs)\n",
    "# train_loss_history_LSTM = history_LSTM.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredict = best_model_LSTM.predict(test_X)\n",
    "print(testPredict.shape)\n",
    "testPredict_scaled = testPredict.ravel()\n",
    "\n",
    "print(testPredict_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_true_scaled = test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recuperando a escala dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll = np.array(target[\"BORE_OIL_VOL\"])\n",
    "\n",
    "meanop = poll.mean()\n",
    "stdop = poll.std()\n",
    "\n",
    "y_test_true = y_test_true_scaled * stdop + meanop\n",
    "testPredict = testPredict_scaled * stdop + meanop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotando a resposta final do processo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "# plt.xlim([1000,1250])\n",
    "plt.ylabel(\"Oil Vol\")\n",
    "plt.xlabel(\"Day\")\n",
    "plt.plot(y_test_true, c = \"g\", alpha = 0.90, linewidth = 2.5,label='Valor Verdadeiro')\n",
    "plt.plot(testPredict, c = \"b\", alpha = 0.75,label='Valor Previsto')\n",
    "plt.legend()\n",
    "plt.title(\"Previsão dos dados de teste\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test_true_scaled, testPredict_scaled))\n",
    "print(\"Test(Validation) RMSE =\"  ,rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test_true, testPredict))\n",
    "print(\"Test(Validation) RMSE =\"  ,rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
